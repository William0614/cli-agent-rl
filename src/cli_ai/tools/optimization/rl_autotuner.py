"""
RL Autotuner for openEuler OS Performance Optimization

This module implements the "Tactician" layer - a reinforcement learning agent that
executes trial-and-error optimization of kernel parameters based on strategies
generated by the LLM "Strategist" layer.

Architecture:
- Custom Gym Environment (OSTuningEnv): Simulates OS parameter tuning
- PPO Agent: Learns optimal parameter configurations
- Real-time feedback: Streams progress to stdout for integration with CLI agent
- Safety mechanisms: Parameter validation, rollback capability, dry-run mode
"""

import argparse
import json
import subprocess
import time
import re
import os
import sys
import platform
from typing import Dict, List, Tuple, Any, Optional
from pathlib import Path
from datetime import datetime

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback

# For system metrics collection
try:
    import cpuinfo
except ImportError:
    cpuinfo = None


class SafetyValidator:
    """Validates parameter values before applying to prevent system instability."""
    
    # Known safe ranges for common kernel parameters
    SAFE_RANGES = {
        'vm.dirty_ratio': (5, 80),
        'vm.dirty_background_ratio': (1, 50),
        'vm.swappiness': (0, 100),
        'net.core.somaxconn': (128, 65535),
        'net.core.netdev_max_backlog': (1000, 100000),
        'net.ipv4.tcp_max_syn_backlog': (128, 65535),
        'kernel.sched_min_granularity_ns': (100000, 10000000),
        'kernel.sched_wakeup_granularity_ns': (100000, 15000000),
    }
    
    @classmethod
    def validate(cls, param_name: str, value: Any, config_min: float, config_max: float) -> Tuple[bool, str]:
        """
        Validate parameter value against known safe ranges.
        
        Returns:
            (is_valid, error_message)
        """
        # Check if value is within config-specified range
        if value < config_min or value > config_max:
            return False, f"Value {value} outside config range [{config_min}, {config_max}]"
        
        # Check against known safe ranges if available
        if param_name in cls.SAFE_RANGES:
            safe_min, safe_max = cls.SAFE_RANGES[param_name]
            if value < safe_min or value > safe_max:
                return False, f"Value {value} outside safe range [{safe_min}, {safe_max}]"
        
        return True, ""


class SystemMetricsCollector:
    """Collects system metrics for the RL state space."""
    
    @staticmethod
    def read_proc_stat() -> Dict[str, float]:
        """Read CPU metrics from /proc/stat."""
        try:
            with open('/proc/stat', 'r') as f:
                line = f.readline()
                fields = line.strip().split()
                
                # CPU time values
                user = float(fields[1])
                nice = float(fields[2])
                system = float(fields[3])
                idle = float(fields[4])
                iowait = float(fields[5]) if len(fields) > 5 else 0.0
                
                total = user + nice + system + idle + iowait
                
                return {
                    'cpu_utilization': ((total - idle) / total) * 100.0 if total > 0 else 0.0,
                    'io_wait': (iowait / total) * 100.0 if total > 0 else 0.0,
                    'system_time': (system / total) * 100.0 if total > 0 else 0.0,
                }
        except Exception as e:
            print(f"Warning: Failed to read /proc/stat: {e}", file=sys.stderr)
            return {'cpu_utilization': 0.0, 'io_wait': 0.0, 'system_time': 0.0}
    
    @staticmethod
    def read_proc_meminfo() -> Dict[str, float]:
        """Read memory metrics from /proc/meminfo."""
        try:
            metrics = {}
            with open('/proc/meminfo', 'r') as f:
                for line in f:
                    if line.startswith('MemTotal:'):
                        metrics['mem_total'] = float(line.split()[1])
                    elif line.startswith('MemAvailable:'):
                        metrics['mem_available'] = float(line.split()[1])
                    elif line.startswith('SwapTotal:'):
                        metrics['swap_total'] = float(line.split()[1])
                    elif line.startswith('SwapFree:'):
                        metrics['swap_free'] = float(line.split()[1])
            
            # Calculate utilization
            if 'mem_total' in metrics and 'mem_available' in metrics:
                metrics['mem_utilization'] = (
                    (metrics['mem_total'] - metrics['mem_available']) / metrics['mem_total']
                ) * 100.0 if metrics['mem_total'] > 0 else 0.0
            
            return metrics
        except Exception as e:
            print(f"Warning: Failed to read /proc/meminfo: {e}", file=sys.stderr)
            return {'mem_utilization': 0.0}
    
    @staticmethod
    def read_sysctl_param(param_name: str) -> Optional[float]:
        """Read current value of a kernel parameter (returns None for non-numeric values)."""
        try:
            result = subprocess.run(
                ['sysctl', '-n', param_name],
                capture_output=True,
                text=True,
                timeout=5
            )
            if result.returncode == 0:
                value_str = result.stdout.strip()
                try:
                    return float(value_str)
                except ValueError:
                    # Skip non-numeric values (e.g., "cubic" for tcp_congestion_control)
                    return None
        except Exception as e:
            print(f"Warning: Failed to read sysctl {param_name}: {e}", file=sys.stderr)
        return None


class OSTuningEnv(gym.Env):
    """
    Custom Gym Environment for OS kernel parameter tuning.
    
    This environment simulates the process of tuning kernel parameters to optimize
    system performance for a specific workload. It uses RL to learn optimal configurations.
    """
    
    metadata = {'render_modes': ['human']}
    
    def __init__(self, config: Dict[str, Any], dry_run: bool = False, verbose: bool = True):
        """
        Initialize the OS Tuning Environment.
        
        Args:
            config: JSON configuration containing:
                - workload_name: Description of the workload
                - reward_metric: Performance metric to maximize
                - benchmark_command: Shell command to run benchmark
                - action_space: List of kernel parameters to tune
                - state_space: List of system metrics to observe
                - training_config: RL training settings
            dry_run: If True, don't actually modify kernel parameters
            verbose: If True, print detailed progress information
        """
        super().__init__()
        
        self.config = config
        self.dry_run = dry_run
        self.verbose = verbose
        
        # Parse configuration
        self.workload_name = config.get('workload_name', 'Unknown Workload')
        self.reward_metric = config.get('reward_metric', 'performance_score')
        self.benchmark_command = config.get('benchmark_command', 'echo "No benchmark specified"')
        self.action_params = config.get('action_space', [])
        self.state_metrics = config.get('state_space', [])
        self.training_config = config.get('training_config', {})
        
        # Performance and stability weights (50/50 split)
        self.performance_weight = 0.5
        self.stability_weight = 0.5
        
        # Store default parameter values for rollback
        self.default_params = {}
        self._store_default_params()
        
        # Define action space (normalized to [0, 1] for all parameters)
        self.action_space = spaces.Box(
            low=0.0,
            high=1.0,
            shape=(len(self.action_params),),
            dtype=np.float32
        )
        
        # Define observation space (system metrics + current parameter values)
        num_metrics = len(self.state_metrics) + len(self.action_params)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(num_metrics,),
            dtype=np.float32
        )
        
        # Tracking
        self.current_step = 0
        self.episode_num = 0
        self.best_reward = -np.inf
        self.best_config = None
        self.baseline_reward = None
        
        # Stability tracking
        self.consecutive_failures = 0
        self.max_consecutive_failures = 3
        
        if self.verbose:
            print(f"\n{'='*80}")
            print(f"RL Autotuner Environment Initialized")
            print(f"{'='*80}")
            print(f"Workload: {self.workload_name}")
            print(f"Reward Metric: {self.reward_metric}")
            print(f"Action Space: {len(self.action_params)} parameters")
            print(f"State Space: {num_metrics} metrics")
            print(f"Dry Run Mode: {self.dry_run}")
            print(f"{'='*80}\n")
    
    def _store_default_params(self):
        """Store current kernel parameter values as defaults for rollback."""
        if self.verbose:
            print("Storing default parameter values...")
        
        for param_config in self.action_params:
            param_name = param_config['param']
            current_value = SystemMetricsCollector.read_sysctl_param(param_name)
            
            if current_value is not None:
                # Store the ACTUAL system value, don't clamp it
                # We need to restore the real original value, not a clamped one
                self.default_params[param_name] = current_value
                if self.verbose:
                    print(f"  {param_name}: {current_value}")
            else:
                # If we can't read, don't store a default
                # Rollback will skip this parameter
                if self.verbose:
                    print(f"  {param_name}: Could not read, will skip rollback")
        
        # If we're in dry-run or couldn't read any params, just log and continue
        if not self.default_params and not self.dry_run:
            print("Warning: Could not read any default parameters. Rollback may not work correctly.")
    
    def _denormalize_action(self, normalized_action: np.ndarray) -> Dict[str, float]:
        """
        Convert normalized action [0, 1] to actual parameter values.
        
        Args:
            normalized_action: Numpy array of normalized values
            
        Returns:
            Dictionary mapping parameter names to actual values
        """
        actual_params = {}
        
        for i, param_config in enumerate(self.action_params):
            param_name = param_config['param']
            min_val = param_config['min']
            max_val = param_config['max']
            param_type = param_config.get('type', 'int')
            
            # Denormalize
            value = min_val + normalized_action[i] * (max_val - min_val)
            
            # Apply type conversion
            if param_type == 'int':
                value = int(round(value))
            
            actual_params[param_name] = value
        
        return actual_params
    
    def _apply_parameters(self, params: Dict[str, float]) -> Tuple[bool, str]:
        """
        Apply kernel parameters to the system.
        
        Args:
            params: Dictionary of parameter names to values
            
        Returns:
            (success, error_message)
        """
        if self.dry_run:
            if self.verbose:
                print("  [DRY RUN] Would apply parameters (not actually applying)")
            return True, ""
        
        for param_name, value in params.items():
            # Find config for this parameter
            param_config = next(
                (p for p in self.action_params if p['param'] == param_name),
                None
            )
            
            if param_config is None:
                return False, f"Parameter {param_name} not found in config"
            
            # Validate
            is_valid, error_msg = SafetyValidator.validate(
                param_name, value, param_config['min'], param_config['max']
            )
            
            if not is_valid:
                return False, f"Validation failed for {param_name}: {error_msg}"
            
            # Apply using sysctl
            try:
                # Use sudo -n (non-interactive) to fail fast if password required
                cmd = ['sudo', '-n', 'sysctl', '-w', f'{param_name}={value}']
                
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                
                if result.returncode != 0:
                    error_msg = f"Failed to apply {param_name}: {result.stderr}"
                    # Check if it's a sudo password issue
                    if "sudo: a password is required" in result.stderr or "sudo: no tty present" in result.stderr:
                        error_msg += "\n\n‚ö†Ô∏è  SUDO PASSWORD REQUIRED ‚ö†Ô∏è"
                        error_msg += "\nThe RL optimizer needs passwordless sudo access."
                        error_msg += "\nTo fix this, run on your openEuler VM:"
                        error_msg += "\n  echo '$(whoami) ALL=(ALL) NOPASSWD: /usr/sbin/sysctl' | sudo tee /etc/sudoers.d/rl-optimizer"
                        error_msg += "\n  sudo chmod 440 /etc/sudoers.d/rl-optimizer"
                    return False, error_msg
                
                if self.verbose:
                    print(f"  ‚úì Applied {param_name}={value}")
                    
            except subprocess.TimeoutExpired:
                return False, f"Timeout applying {param_name}"
            except Exception as e:
                return False, f"Error applying {param_name}: {e}"
        
        return True, ""
    
    def _run_benchmark(self) -> Tuple[Optional[float], bool, str]:
        """
        Run the benchmark command and extract the reward metric.
        
        Returns:
            (reward, success, error_message)
        """
        if self.verbose:
            print(f"  Running benchmark: {self.benchmark_command}")
        
        try:
            start_time = time.time()
            timeout = self.training_config.get('benchmark_timeout', 120)
            
            result = subprocess.run(
                self.benchmark_command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=timeout
            )
            
            elapsed_time = time.time() - start_time
            
            if result.returncode != 0:
                error_msg = f"Benchmark failed with exit code {result.returncode}: {result.stderr}"
                return None, False, error_msg
            
            # Parse output to extract reward metric
            reward = self._parse_reward_from_output(result.stdout)
            
            if reward is None:
                error_msg = f"Could not parse {self.reward_metric} from benchmark output"
                return None, False, error_msg
            
            if self.verbose:
                print(f"  ‚úì Benchmark completed in {elapsed_time:.2f}s: {self.reward_metric}={reward}")
            
            return reward, True, ""
            
        except subprocess.TimeoutExpired:
            return None, False, "Benchmark timeout"
        except Exception as e:
            return None, False, f"Benchmark error: {e}"
    
    def _parse_reward_from_output(self, output: str) -> Optional[float]:
        """
        Parse the reward metric from benchmark output.
        
        This uses regex patterns to extract numeric values associated with the metric name.
        """
        # Try to find metric in output
        patterns = [
            rf'{self.reward_metric}[:\s]+([0-9.]+)',  # "metric: 123.45"
            rf'([0-9.]+)\s+{self.reward_metric}',      # "123.45 metric"
            r'([0-9.]+)',                                # Just a number (last resort)
        ]
        
        for pattern in patterns:
            match = re.search(pattern, output, re.IGNORECASE)
            if match:
                try:
                    return float(match.group(1))
                except ValueError:
                    continue
        
        return None
    
    def _get_observation(self) -> np.ndarray:
        """
        Collect current system state.
        
        Returns:
            Numpy array of normalized state values
        """
        observation = []
        
        # Collect system metrics
        cpu_metrics = SystemMetricsCollector.read_proc_stat()
        mem_metrics = SystemMetricsCollector.read_proc_meminfo()
        
        for metric_config in self.state_metrics:
            metric_name = metric_config['metric']
            
            # Get value from appropriate source
            if metric_name in cpu_metrics:
                value = cpu_metrics[metric_name]
            elif metric_name in mem_metrics:
                value = mem_metrics[metric_name]
            else:
                value = 0.0
            
            observation.append(value)
        
        # Add current parameter values (normalized)
        for param_config in self.action_params:
            param_name = param_config['param']
            current_value = SystemMetricsCollector.read_sysctl_param(param_name)
            
            if current_value is not None:
                # Normalize to [0, 1]
                min_val = param_config['min']
                max_val = param_config['max']
                normalized = (current_value - min_val) / (max_val - min_val) if max_val > min_val else 0.5
                observation.append(normalized)
            else:
                observation.append(0.5)  # Default to midpoint if can't read
        
        return np.array(observation, dtype=np.float32)
    
    def _calculate_stability_score(self) -> float:
        """
        Calculate a stability score based on system health indicators.
        
        Returns:
            Stability score between 0 and 1 (higher is more stable)
        """
        cpu_metrics = SystemMetricsCollector.read_proc_stat()
        mem_metrics = SystemMetricsCollector.read_proc_meminfo()
        
        # Penalize high I/O wait and memory pressure
        io_wait_penalty = max(0, 1.0 - (cpu_metrics.get('io_wait', 0) / 50.0))
        mem_pressure_penalty = max(0, 1.0 - (mem_metrics.get('mem_utilization', 0) / 100.0))
        
        # Average of stability indicators
        stability = (io_wait_penalty + mem_pressure_penalty) / 2.0
        
        return stability
    
    def reset(self, seed: Optional[int] = None, options: Optional[Dict] = None) -> Tuple[np.ndarray, Dict]:
        """
        Reset the environment to initial state.
        
        This restores default kernel parameters and gets initial observation.
        """
        super().reset(seed=seed)
        
        self.episode_num += 1
        self.current_step = 0
        self.consecutive_failures = 0
        
        if self.verbose:
            print(f"\n{'='*80}")
            print(f"Episode {self.episode_num} - Reset")
            print(f"{'='*80}")
        
        # Restore default parameters (silently - use rollback instead of _apply_parameters)
        self.rollback()
        
        # Get baseline reward on first reset
        if self.baseline_reward is None and not self.dry_run:
            if self.verbose:
                print("Measuring baseline performance...")
            baseline, success, error = self._run_benchmark()
            if success:
                self.baseline_reward = baseline
                if self.verbose:
                    print(f"  Baseline {self.reward_metric}: {baseline}")
            else:
                if self.verbose:
                    print(f"  Warning: Could not measure baseline: {error}")
        
        observation = self._get_observation()
        info = {'episode': self.episode_num}
        
        return observation, info
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """
        Execute one step of the environment.
        
        This is the core RL loop:
        1. Apply action (set kernel parameters)
        2. Run benchmark
        3. Calculate reward
        4. Get new observation
        
        Args:
            action: Normalized action from RL agent
            
        Returns:
            (observation, reward, terminated, truncated, info)
        """
        self.current_step += 1
        
        if self.verbose:
            print(f"\n--- Step {self.current_step} ---")
        
        # Denormalize action to actual parameter values
        params = self._denormalize_action(action)
        
        if self.verbose:
            print("Action (parameters to apply):")
            for param, value in params.items():
                print(f"  {param}: {value}")
        
        # Apply parameters
        success, error = self._apply_parameters(params)
        
        if not success:
            if self.verbose:
                print(f"  ‚úó Failed to apply parameters: {error}")
            
            self.consecutive_failures += 1
            
            # Return negative reward and check if we should terminate
            observation = self._get_observation()
            terminated = self.consecutive_failures >= self.max_consecutive_failures
            
            return observation, -100.0, terminated, False, {'error': error}
        
        # Run benchmark
        performance_score, success, error = self._run_benchmark()
        
        if not success:
            if self.verbose:
                print(f"  ‚úó Benchmark failed: {error}")
            
            self.consecutive_failures += 1
            
            observation = self._get_observation()
            terminated = self.consecutive_failures >= self.max_consecutive_failures
            
            return observation, -100.0, terminated, False, {'error': error}
        
        # Calculate stability score
        stability_score = self._calculate_stability_score()
        
        # Calculate combined reward (50% performance, 50% stability)
        performance_reward = performance_score
        stability_reward = stability_score * 100.0  # Scale to similar magnitude
        
        reward = (
            self.performance_weight * performance_reward +
            self.stability_weight * stability_reward
        )
        
        # Reset failure counter on success
        self.consecutive_failures = 0
        
        # Track best configuration
        if reward > self.best_reward:
            self.best_reward = reward
            self.best_config = params.copy()
            if self.verbose:
                print(f"  üéØ New best reward: {reward:.2f} (performance: {performance_score:.2f}, stability: {stability_score:.2f})")
        
        if self.verbose:
            print(f"  Reward: {reward:.2f} (perf: {performance_reward:.2f}, stab: {stability_reward:.2f})")
            if self.baseline_reward:
                improvement = ((performance_score - self.baseline_reward) / self.baseline_reward) * 100
                print(f"  Improvement over baseline: {improvement:+.2f}%")
        
        # Get new observation
        observation = self._get_observation()
        
        # Episode termination logic
        max_steps = self.training_config.get('max_steps_per_episode', 50)
        terminated = self.current_step >= max_steps
        truncated = False
        
        info = {
            'performance_score': performance_score,
            'stability_score': stability_score,
            'params': params,
            'step': self.current_step
        }
        
        return observation, reward, terminated, truncated, info
    
    def rollback(self):
        """Emergency rollback to default parameters (silent, best-effort)."""
        if not self.default_params:
            return  # Nothing to restore
            
        # Try to restore each parameter individually
        # Completely silent - don't show any warnings
        for param_name, value in self.default_params.items():
            try:
                if not self.dry_run:
                    subprocess.run(
                        ['sudo', 'sysctl', '-w', f'{param_name}={int(value)}'],
                        capture_output=True,
                        text=True,
                        timeout=5,
                        check=False  # Don't raise exception on non-zero exit
                    )
            except Exception:
                pass  # Silently ignore all errors - rollback is best-effort
    
    def get_best_config(self) -> Tuple[Dict[str, float], float]:
        """Get the best configuration found so far."""
        return self.best_config, self.best_reward


class RLProgressCallback(BaseCallback):
    """Callback for real-time progress reporting and visualization updates."""
    
    def __init__(self, env: OSTuningEnv, dashboard=None, web_dashboard=None, console_tracker=None, verbose: int = 0):
        super().__init__(verbose)
        self.env = env
        self.dashboard = dashboard
        self.web_dashboard = web_dashboard
        self.console_tracker = console_tracker
        self.episode_rewards = []
        self.episode_lengths = []
        self.step_count = 0
    
    def _on_step(self) -> bool:
        """Called after each step in the environment."""
        self.step_count += 1
        
        # Get info from the last step
        info = self.locals.get('infos', [{}])[0]
        
        if 'performance_score' in info and 'stability_score' in info:
            reward = self.locals.get('rewards', [0])[0]
            performance = info['performance_score']
            stability = info['stability_score']
            episode = self.env.episode_num
            params = info.get('params', {})
            
            # Update web dashboard if available (priority)
            if self.web_dashboard is not None:
                try:
                    self.web_dashboard.add_data_point(
                        step=self.step_count,
                        reward=reward,
                        performance=performance,
                        stability=stability,
                        episode=episode,
                        params=params
                    )
                except Exception as e:
                    if self.verbose > 0:
                        print(f"Web dashboard update error: {e}")
            
            # Update GUI dashboard if available
            elif self.dashboard is not None:
                try:
                    self.dashboard.add_data_point(
                        step=self.step_count,
                        reward=reward,
                        performance=performance,
                        stability=stability,
                        episode=episode,
                        params=params
                    )
                except Exception as e:
                    if self.verbose > 0:
                        print(f"Dashboard update error: {e}")
            
            # Update console tracker if available (when both dashboards fail)
            elif self.console_tracker is not None:
                try:
                    self.console_tracker.update(
                        step=self.step_count,
                        reward=reward,
                        performance=performance,
                        stability=stability,
                        episode=episode,
                        params=params
                    )
                except Exception as e:
                    if self.verbose > 0:
                        print(f"Console tracker error: {e}")
        
        return True
    
    def _on_rollout_end(self) -> None:
        """Called at the end of a rollout."""
        if self.verbose > 0:
            best_config, best_reward = self.env.get_best_config()
            if best_config:
                print(f"\n{'='*80}")
                print(f"Rollout Summary")
                print(f"{'='*80}")
                print(f"Best Reward So Far: {best_reward:.2f}")
                print(f"Steps: {self.step_count}")
                print(f"Episodes: {self.env.episode_num}")
                print(f"{'='*80}\n")


def run_rl_optimization(config_path: str, dry_run: bool = False, verbose: bool = False, 
                       show_dashboard: bool = True, use_web_dashboard: bool = False,
                       web_host: str = '0.0.0.0', web_port: int = 5000) -> Dict[str, Any]:
    """
    Main function to run RL-based OS optimization.
    
    Args:
        config_path: Path to JSON configuration file
        dry_run: If True, simulate without actually modifying system
        verbose: If True, print detailed progress (default: False)
        show_dashboard: If True, show real-time visualization dashboard (default: True)
        use_web_dashboard: If True, use web-based dashboard instead of GUI (default: False)
        web_host: Host address for web dashboard (default: '0.0.0.0')
        web_port: Port for web dashboard (default: 5000)
        
    Returns:
        Dictionary containing optimization results
    """
    # Load configuration
    try:
        with open(config_path, 'r') as f:
            config = json.load(f)
    except Exception as e:
        print(f"ERROR: Failed to load config from {config_path}: {e}", file=sys.stderr)
        return {'success': False, 'error': str(e)}
    
    # Validate config
    required_keys = ['workload_name', 'reward_metric', 'benchmark_command', 'action_space', 'state_space']
    for key in required_keys:
        if key not in config:
            print(f"ERROR: Missing required config key: {key}", file=sys.stderr)
            return {'success': False, 'error': f'Missing key: {key}'}
    
    # Check sudo access (critical for RL optimization)
    if not dry_run:
        print("Checking sudo access for sysctl...", file=sys.stderr)
        try:
            test_result = subprocess.run(
                ['sudo', '-n', 'sysctl', 'vm.swappiness'],
                capture_output=True,
                text=True,
                timeout=5
            )
            if test_result.returncode != 0:
                error_msg = "\n" + "="*80 + "\n"
                error_msg += "‚ö†Ô∏è  SUDO ACCESS REQUIRED ‚ö†Ô∏è\n"
                error_msg += "="*80 + "\n"
                error_msg += "The RL optimizer needs passwordless sudo for sysctl.\n"
                error_msg += "Without this, the system will hang waiting for password.\n\n"
                error_msg += "To fix this on your openEuler VM, run:\n"
                error_msg += f"  echo '$(whoami) ALL=(ALL) NOPASSWD: /usr/sbin/sysctl' | sudo tee /etc/sudoers.d/rl-optimizer\n"
                error_msg += f"  sudo chmod 440 /etc/sudoers.d/rl-optimizer\n\n"
                error_msg += "Then verify with: sudo -n sysctl vm.swappiness\n"
                error_msg += "="*80
                print(error_msg, file=sys.stderr)
                return {'success': False, 'error': 'Sudo access not configured. See error message above.'}
            print("‚úì Sudo access verified\n", file=sys.stderr)
        except subprocess.TimeoutExpired:
            print("ERROR: Sudo check timed out (likely waiting for password)", file=sys.stderr)
            return {'success': False, 'error': 'Sudo access check timed out'}
        except Exception as e:
            print(f"Warning: Could not verify sudo access: {e}", file=sys.stderr)
    
    # Create environment
    try:
        env = OSTuningEnv(config, dry_run=dry_run, verbose=verbose)
    except Exception as e:
        print(f"ERROR: Failed to create environment: {e}", file=sys.stderr)
        return {'success': False, 'error': str(e)}
    
    # Get training configuration
    training_config = config.get('training_config', {})
    total_timesteps = training_config.get('total_timesteps', 10000)
    learning_rate = training_config.get('learning_rate', 3e-4)
    
    print(f"\n{'='*80}")
    print(f"Starting RL Training")
    print(f"{'='*80}")
    print(f"Total Timesteps: {total_timesteps}")
    print(f"Learning Rate: {learning_rate}")
    print(f"Visualization: {'Web Dashboard' if use_web_dashboard else ('GUI Dashboard' if show_dashboard else 'Disabled')}")
    if use_web_dashboard:
        print(f"Dashboard URL: http://{web_host}:{web_port}")
    print(f"{'='*80}\n")
    
    # Initialize visualization dashboard
    dashboard = None
    web_dashboard = None
    console_tracker = None
    dashboard_thread = None
    web_server_thread = None
    
    if use_web_dashboard and show_dashboard:
        # Use web-based dashboard for headless servers
        try:
            from .web_dashboard import WebDashboard, run_dashboard_server
            import threading
            
            param_names = [p['param'] for p in config['action_space']]
            web_dashboard = WebDashboard(
                workload_name=config['workload_name'],
                param_names=param_names,
                config=config
            )
            
            # Start Flask server in background thread
            web_server_thread = threading.Thread(
                target=run_dashboard_server,
                kwargs={'host': web_host, 'port': web_port},
                daemon=True
            )
            web_server_thread.start()
            
            print("‚úì Web-based dashboard server started")
            print(f"  ‚Üí Open http://{web_host}:{web_port} in your browser")
            print("  - Real-time learning curve")
            print("  - Performance vs Stability")
            print("  - Episode rewards")
            print("  - Best configuration tracker\n")
            
            time.sleep(2)  # Give server time to start
            
        except Exception as e:
            print(f"Warning: Could not initialize web dashboard: {e}")
            print("Falling back to console progress tracker...\n")
            web_dashboard = None
            
            try:
                from .console_progress import ConsoleProgressTracker
                console_tracker = ConsoleProgressTracker()
            except:
                pass
    
    elif show_dashboard:
        # Use GUI dashboard (requires display)
        try:
            from .visualization import create_dashboard
            param_names = [p['param'] for p in config['action_space']]
            dashboard = create_dashboard(param_names)
            dashboard.start()
            
            # Start update loop in separate thread
            import threading
            dashboard_thread = threading.Thread(target=dashboard.run_update_loop, daemon=True)
            dashboard_thread.start()
            
            print("‚úì Real-time visualization dashboard started")
            print("  - Learning curve")
            print("  - Performance vs Stability")
            print("  - Parameter exploration")
            print("  - Best configuration tracker\n")
        except Exception as e:
            print(f"Warning: Could not initialize dashboard: {e}")
            print("Using console progress tracker instead...\n")
            dashboard = None
            
            # Fall back to console tracker
            try:
                from .console_progress import ConsoleProgressTracker
                console_tracker = ConsoleProgressTracker()
            except:
                pass
    
    # Create PPO agent
    try:
        model = PPO(
            "MlpPolicy",
            env,
            learning_rate=learning_rate,
            n_steps=training_config.get('n_steps', 2048),
            batch_size=training_config.get('batch_size', 64),
            n_epochs=training_config.get('n_epochs', 10),
            gamma=training_config.get('gamma', 0.99),
            verbose=1 if verbose else 0
        )
    except Exception as e:
        print(f"ERROR: Failed to create PPO model: {e}", file=sys.stderr)
        env.rollback()
        return {'success': False, 'error': str(e)}
    
    # Train the model
    try:
        callback = RLProgressCallback(
            env, 
            dashboard=dashboard,
            web_dashboard=web_dashboard,
            console_tracker=console_tracker,
            verbose=1 if verbose else 0
        )
        model.learn(total_timesteps=total_timesteps, callback=callback)
    except KeyboardInterrupt:
        print("\n\nTraining interrupted by user. Performing rollback...", file=sys.stderr)
        env.rollback()
        if web_dashboard:
            print("Web dashboard server will continue running. Press Ctrl+C again to stop.")
        if dashboard:
            dashboard.stop()
        if console_tracker:
            console_tracker.print_summary()
        return {'success': False, 'error': 'Interrupted by user'}
    except Exception as e:
        print(f"\nERROR during training: {e}", file=sys.stderr)
        env.rollback()
        if dashboard:
            dashboard.stop()
        if console_tracker:
            console_tracker.print_summary()
        return {'success': False, 'error': str(e)}
    finally:
        # Keep dashboard open for a few seconds to see final state
        if web_dashboard:
            print("\nTraining complete! Web dashboard is still accessible.")
            print(f"View results at: http://{web_host}:{web_port}")
            print("Dashboard will remain running. Press Ctrl+C to stop.")
        elif dashboard:
            print("\nTraining complete! Dashboard will remain open for 10 seconds...")
            time.sleep(10)
            dashboard.stop()
        elif console_tracker:
            console_tracker.print_summary()
    
    # Get final results
    best_config, best_reward = env.get_best_config()
    
    results = {
        'success': True,
        'workload_name': config['workload_name'],
        'best_reward': float(best_reward) if best_reward != -np.inf else None,
        'best_config': best_config,
        'baseline_reward': float(env.baseline_reward) if env.baseline_reward else None,
        'improvement': (
            ((best_reward - env.baseline_reward) / env.baseline_reward * 100)
            if env.baseline_reward and best_reward != -np.inf else None
        ),
        'total_episodes': env.episode_num
    }
    
    # Print final summary
    print(f"\n{'='*80}")
    print(f"Optimization Complete!")
    print(f"{'='*80}")
    print(f"Workload: {results['workload_name']}")
    print(f"Total Episodes: {results['total_episodes']}")
    
    if results['baseline_reward']:
        print(f"Baseline Performance: {results['baseline_reward']:.2f}")
    
    if results['best_reward']:
        print(f"Best Performance: {results['best_reward']:.2f}")
        if results['improvement']:
            print(f"Improvement: {results['improvement']:+.2f}%")
    
    if best_config:
        print(f"\nOptimal Configuration:")
        for param, value in best_config.items():
            print(f"  {param} = {value}")
        print(f"\nTo apply this configuration permanently, run:")
        for param, value in best_config.items():
            print(f"  sudo sysctl -w {param}={value}")
    
    print(f"{'='*80}\n")
    
    return results


def main():
    """Main entry point for standalone script execution."""
    parser = argparse.ArgumentParser(
        description='RL Autotuner for OS Performance Optimization'
    )
    parser.add_argument(
        '--config',
        type=str,
        required=True,
        help='Path to JSON configuration file'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='Simulate optimization without modifying system'
    )
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='Suppress verbose output'
    )
    
    args = parser.parse_args()
    
    # Run optimization
    results = run_rl_optimization(
        config_path=args.config,
        dry_run=args.dry_run,
        verbose=not args.quiet
    )
    
    # Exit with appropriate code
    sys.exit(0 if results['success'] else 1)


if __name__ == '__main__':
    main()
